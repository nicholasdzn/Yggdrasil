{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_texto_da_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            texto = soup.get_text()\n",
    "            return texto\n",
    "        else:\n",
    "            print(f\"Erro ao acessar a URL. Código de status: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao acessar a URL: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "url = 'https://brasil.un.org/pt-br/91863-agenda-2030-para-o-desenvolvimento-sustent%C3%A1vel'\n",
    "\n",
    "texto_extraido = extrair_texto_da_url(url)\n",
    "\n",
    "if texto_extraido:\n",
    "    \n",
    "    with open('texto_extraido.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(texto_extraido)\n",
    "    \n",
    "    print(\"Texto extraído com sucesso e salvo no arquivo 'texto_extraido.txt'.\")\n",
    "else:\n",
    "    print(\"Não foi possível extrair o texto da URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessar_texto(texto):\n",
    "    # Tokenização usando o Tokenizador BERT (pode ser adaptado para outros tokenizadores)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    \n",
    "    # Remoção de caracteres especiais e números\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    \n",
    "    # Remoção de stop words (exemplo com stop words em inglês)\n",
    "    stop_words = set([\"a\", \"an\", \"the\", \"in\", \"on\", \"at\"])  # Adicione mais palavras conforme necessário\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Junção dos tokens de volta em uma string\n",
    "    texto_processado = ' '.join(tokens)\n",
    "    \n",
    "    return texto_processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('texto_extraido.txt') as file:\n",
    "#     text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed = preprocessar_texto(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Carrega o modelo GPT-2 e o tokenizador\n",
    "model_name = \"gpt2\"  # Pode ser \"gpt2-medium\", \"gpt2-large\", etc., para modelos maiores\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Texto de contexto\n",
    "# contexto = \"O aquecimento global é um fenômeno preocupante que está afetando o clima global.\"\n",
    "# contexto = \"Global warming is a worrisome phenomenon that is affecting the global climate.\"\n",
    "contexto = \"This Agenda is a plan of action for people, for the planet and for prosperity.\"\n",
    "\n",
    "# Gera uma pergunta com base no contexto\n",
    "input_text = f\"Generate a question about: '{contexto}'\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Gera a pergunta\n",
    "with torch.no_grad():\n",
    "    resposta = model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=0)\n",
    "\n",
    "# Decodifica e exibe a pergunta gerada\n",
    "pergunta_gerada = tokenizer.decode(np.squeeze(resposta), skip_special_tokens=True)\n",
    "\n",
    "print(\"Pergunta Gerada:\")\n",
    "print(pergunta_gerada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2ForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2ForQuestionAnswering.from_pretrained(\"gpt2\")\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "\n",
    "# target is \"nice puppet\"\n",
    "target_start_index = torch.tensor([14])\n",
    "target_end_index = torch.tensor([15])\n",
    "\n",
    "outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n",
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Carregar o modelo pré-treinado GPT-3.5 e o tokenizador\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"  # Este é um modelo GPT-3.5-like\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Definir o contexto\n",
    "context = \"O presidente dos Estados Unidos é Joe Biden. Ele foi eleito em 2020.\"\n",
    "\n",
    "# Tokenizar o contexto\n",
    "input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "\n",
    "# Gerar perguntas\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=50256)\n",
    "\n",
    "# Decodificar a saída em texto legível\n",
    "question = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Pergunta gerada: \", question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
